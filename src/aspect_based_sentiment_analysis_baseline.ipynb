{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11085,
     "status": "ok",
     "timestamp": 1662693657879,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "_RDH-YQsQ2K0",
    "outputId": "d919c20a-e4e0-4b02-eb85-290d0b72798f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 5.2 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 52.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 39.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7479,
     "status": "ok",
     "timestamp": 1662693710729,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "gmtfOHQSQ-nS",
    "outputId": "2045753e-7c0d-4d97-f317-52f3011f470c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[K     |████████████████████████████████| 365 kB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 47.3 MB/s \n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 59.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 69.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "Successfully installed datasets-2.4.0 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "urllib3"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jprcZhTyetFF"
   },
   "source": [
    "# 모듈 import 및 전역 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1662696688692,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "8LDwADWNMpsC"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from transformers import XLMRobertaModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "PADDING_TOKEN = 1\n",
    "S_OPEN_TOKEN = 0\n",
    "S_CLOSE_TOKEN = 2\n",
    "\n",
    "do_eval=True\n",
    "\n",
    "category_extraction_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/category_extraction/'\n",
    "polarity_classification_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/polarity_classification/'\n",
    "\n",
    "test_category_extraction_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/category_extraction/saved_model_example.pt'\n",
    "test_polarity_classification_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/polarity_classification/saved_model_example.pt'\n",
    "\n",
    "train_data_path = '/content/drive/MyDrive/korean_baseline/data/sample.jsonl'\n",
    "dev_data_path = '/content/drive/MyDrive/korean_baseline/data/sample.jsonl'\n",
    "test_data_path = '/content/drive/MyDrive/korean_baseline/data/sample.jsonl'\n",
    "\n",
    "max_len = 256\n",
    "batch_size = 8\n",
    "base_model = 'xlm-roberta-base'\n",
    "learning_rate = 3e-6\n",
    "eps = 1e-8\n",
    "num_train_epochs = 20\n",
    "classifier_hidden_size = 768\n",
    "classifier_dropout_prob = 0.1\n",
    "\n",
    "entity_property_pair = [\n",
    "    '제품 전체#일반', '제품 전체#가격', '제품 전체#디자인', '제품 전체#품질', '제품 전체#편의성', '제품 전체#인지도',\n",
    "    '본품#일반', '본품#디자인', '본품#품질', '본품#편의성', '본품#다양성',\n",
    "    '패키지/구성품#일반', '패키지/구성품#디자인', '패키지/구성품#품질', '패키지/구성품#편의성', '패키지/구성품#다양성',\n",
    "    '브랜드#일반', '브랜드#가격', '브랜드#디자인', '브랜드#품질', '브랜드#인지도',\n",
    "                    ]\n",
    "\n",
    "tf_id_to_name = ['True', 'False']\n",
    "tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n",
    "\n",
    "polarity_id_to_name = ['positive', 'negative', 'neutral']\n",
    "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGmH15hCeqhJ"
   },
   "source": [
    "json 및 jsonl 파일 read, write 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1662696697330,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "6vGeHU4yP2Sg",
    "outputId": "30df52ca-3173-4dfc-a56d-1325650f5c6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'nikluge-sa-2022-train-00001',\n",
       "  'sentence_form': '둘쨋날은 미친듯이 밟아봤더니 기어가 헛돌면서 틱틱 소리가 나서 경악.',\n",
       "  'annotation': [['본품#품질', ['기어', 16, 18], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00002',\n",
       "  'sentence_form': '이거 뭐 삐꾸를 준 거 아냐 불안하고, 거금 투자한 게 왜 이래.. 싶어서 정이 확 떨어졌는데 산 곳 가져가서 확인하니 기어 텐션 문제라고 고장 아니래.',\n",
       "  'annotation': [['본품#품질', ['기어 텐션', 67, 72], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00003',\n",
       "  'sentence_form': '간사하게도 그 이후에는 라이딩이 아주 즐거워져서 만족스럽게 탔다.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00004',\n",
       "  'sentence_form': '샥이 없는 모델이라 일반 도로에서 타면 노면의 진동 때문에 손목이 덜덜덜 떨리고 이가 부딪칠 지경인데 이마저도 며칠 타면서 익숙해지니 신경쓰이지 않게 됐다.',\n",
       "  'annotation': [['제품 전체#일반', ['샥이 없는 모델', 0, 8], 'neutral']]},\n",
       " {'id': 'nikluge-sa-2022-train-00005',\n",
       "  'sentence_form': '안장도 딱딱해서 엉덩이가 아팠는데 무시하고 타고 있다.',\n",
       "  'annotation': [['본품#일반', ['안장', 0, 2], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00006',\n",
       "  'sentence_form': '지금 내 실력과 저질 체력으로는 이 정도 자전거도 되게 훌륭한 거라는..',\n",
       "  'annotation': [['제품 전체#일반', ['자전거', 23, 26], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00007',\n",
       "  'sentence_form': '내장 기어 3단은 썩 좋은 물건이라 기어 변환도 부드럽고 겉에서는 기어가 보이지 않기 때문에 깔끔하다.',\n",
       "  'annotation': [['본품#품질', ['내장 기어 3단', 0, 8], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00008',\n",
       "  'sentence_form': '한번 교환했는데 새로 온 UD20은 불량화소가 있고 ㅜ ㅜ ㅜ',\n",
       "  'annotation': [['본품#품질', ['UD20', 14, 18], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00009',\n",
       "  'sentence_form': '전에 작동 안되었던 자막 검색 후 등록 기능이 똑같이 작동 안 된다!!!',\n",
       "  'annotation': [['본품#품질', ['자막 검색 후 등록 기능', 11, 24], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00010',\n",
       "  'sentence_form': '왜 [등록]키를 만들어놓고 제대로 단어장에 등록이 되지 않는 거냐!!',\n",
       "  'annotation': [['본품#품질', ['등록]키', 3, 7], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00011',\n",
       "  'sentence_form': '다른 부가 기능은 참 훌륭한데..',\n",
       "  'annotation': [['본품#품질', ['부가 기능', 3, 8], 'positive']]},\n",
       " {'id': 'nikluge-sa-2022-train-00012',\n",
       "  'sentence_form': '미치겠네.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00013',\n",
       "  'sentence_form': '아.. 진짜 기계 사겠나.',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00014',\n",
       "  'sentence_form': '이번에는 사전까지..',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]},\n",
       " {'id': 'nikluge-sa-2022-train-00015',\n",
       "  'sentence_form': '이런 젠장..',\n",
       "  'annotation': [['제품 전체#일반', [None, 0, 0], 'negative']]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jsonload(fname, encoding=\"utf-8\"):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    return j\n",
    "\n",
    "\n",
    "# json 개체를 파일이름으로 깔끔하게 저장\n",
    "def jsondump(j, fname):\n",
    "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False)\n",
    "\n",
    "# jsonl 파일 읽어서 list에 저장\n",
    "def jsonlload(fname, encoding=\"utf-8\"):\n",
    "    json_list = []\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        for line in f.readlines():\n",
    "            json_list.append(json.loads(line))\n",
    "    return json_list\n",
    "\n",
    "jsonlload('/content/drive/MyDrive/korean_baseline/data/sample.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuHV8HGqXvg_"
   },
   "source": [
    "# 모델 정의\n",
    "xlm-roberta 모델을 기반으로 한 classification 모델 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1662696703007,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "WkyrAEhAQBQV"
   },
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_label):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(classifier_hidden_size, classifier_hidden_size)\n",
    "        self.dropout = nn.Dropout(classifier_dropout_prob)\n",
    "        self.output = nn.Linear(classifier_hidden_size, num_label)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RoBertaBaseClassifier(nn.Module):\n",
    "    def __init__(self, num_label, len_tokenizer):\n",
    "        super(RoBertaBaseClassifier, self).__init__()\n",
    "\n",
    "        self.num_label = num_label\n",
    "        self.xlm_roberta = XLMRobertaModel.from_pretrained(base_model)\n",
    "        self.xlm_roberta.resize_token_embeddings(len_tokenizer)\n",
    "\n",
    "        self.labels_classifier = SimpleClassifier(self.num_label)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.xlm_roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.labels_classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_label),\n",
    "                                                labels.view(-1))\n",
    "\n",
    "        return loss, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-p0LKeGi194"
   },
   "source": [
    "# 데이터 파싱 및 tokenization 함수 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1662696707633,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "XO-hv7lQQGA5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n",
    "\n",
    "    entity_property_data_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': []\n",
    "    }\n",
    "    polarity_data_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    for pair in entity_property_pair:\n",
    "        isPairInOpinion = False\n",
    "        if pd.isna(form):\n",
    "            break\n",
    "        tokenized_data = tokenizer(form, pair, padding='max_length', max_length=max_len, truncation=True)\n",
    "        for annotation in annotations:\n",
    "            entity_property = annotation[0]\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if polarity == '------------':\n",
    "                continue\n",
    "\n",
    "            if entity_property == pair:\n",
    "                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "                entity_property_data_dict['label'].append(tf_name_to_id['True'])\n",
    "\n",
    "                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "                polarity_data_dict['label'].append(polarity_name_to_id[polarity])\n",
    "\n",
    "                isPairInOpinion = True\n",
    "                break\n",
    "\n",
    "        if isPairInOpinion is False:\n",
    "            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
    "            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
    "            entity_property_data_dict['label'].append(tf_name_to_id['False'])\n",
    "\n",
    "    return entity_property_data_dict, polarity_data_dict\n",
    "\n",
    "\n",
    "def get_dataset(raw_data, tokenizer, max_len):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_labels_list = []\n",
    "\n",
    "    polarity_input_ids_list = []\n",
    "    polarity_attention_mask_list = []\n",
    "    polarity_token_labels_list = []\n",
    "\n",
    "    for utterance in raw_data:\n",
    "        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)\n",
    "        input_ids_list.extend(entity_property_data_dict['input_ids'])\n",
    "        attention_mask_list.extend(entity_property_data_dict['attention_mask'])\n",
    "        token_labels_list.extend(entity_property_data_dict['label'])\n",
    "\n",
    "        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])\n",
    "        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])\n",
    "        polarity_token_labels_list.extend(polarity_data_dict['label'])\n",
    "\n",
    "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
    "                         torch.tensor(token_labels_list)), TensorDataset(torch.tensor(polarity_input_ids_list), torch.tensor(polarity_attention_mask_list),\n",
    "                         torch.tensor(polarity_token_labels_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ka98lsxi--Y"
   },
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1662696779211,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "YL0yp7zSaX9B"
   },
   "outputs": [],
   "source": [
    "def evaluation(y_true, y_pred, label_len):\n",
    "    count_list = [0]*label_len\n",
    "    hit_list = [0]*label_len\n",
    "    for i in range(len(y_true)):\n",
    "        count_list[y_true[i]] += 1\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            hit_list[y_true[i]] += 1\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(label_len):\n",
    "        acc_list.append(hit_list[i]/count_list[i])\n",
    "\n",
    "    print(count_list)\n",
    "    print(hit_list)\n",
    "    print(acc_list)\n",
    "    print('accuracy: ', (sum(hit_list) / sum(count_list)))\n",
    "    print('macro_accuracy: ', sum(acc_list) / 3)\n",
    "    # print(y_true)\n",
    "\n",
    "    y_true = list(map(int, y_true))\n",
    "    y_pred = list(map(int, y_pred))\n",
    "\n",
    "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
    "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n",
    "    print('f1_score_macro: ', f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "def train_sentiment_analysis():\n",
    "\n",
    "    print('train_sentiment_analysis')\n",
    "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
    "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
    "\n",
    "    print('loading train data')\n",
    "    train_data = jsonlload(train_data_path)\n",
    "    dev_data = jsonlload(dev_data_path)\n",
    "\n",
    "    print('tokenizing train data')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print('We have added', num_added_toks, 'tokens')\n",
    "    entity_property_train_data, polarity_train_data = get_dataset(train_data, tokenizer, max_len)\n",
    "    entity_property_dev_data, polarity_dev_data = get_dataset(dev_data, tokenizer, max_len)\n",
    "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
    "                                  batch_size=batch_size)\n",
    "    entity_property_dev_dataloader = DataLoader(entity_property_dev_data, shuffle=True,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
    "                                                  batch_size=batch_size)\n",
    "    polarity_dev_dataloader = DataLoader(polarity_dev_data, shuffle=True,\n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "    print('loading model')\n",
    "    entity_property_model = RoBertaBaseClassifier(len(tf_id_to_name), len(tokenizer))\n",
    "    entity_property_model.to(device)\n",
    "\n",
    "    polarity_model = RoBertaBaseClassifier(len(polarity_id_to_name), len(tokenizer))\n",
    "    polarity_model.to(device)\n",
    "\n",
    "\n",
    "    print('end loading')\n",
    "\n",
    "    # entity_property_model_optimizer_setting\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        entity_property_optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
    "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
    "\n",
    "    entity_property_optimizer = AdamW(\n",
    "        entity_property_optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=eps\n",
    "    )\n",
    "    epochs = num_train_epochs\n",
    "    max_grad_norm = 1.0\n",
    "    total_steps = epochs * len(entity_property_train_dataloader)\n",
    "\n",
    "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
    "        entity_property_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # polarity_model_optimizer_setting\n",
    "    if FULL_FINETUNING:\n",
    "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        polarity_optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
    "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
    "\n",
    "    polarity_optimizer = AdamW(\n",
    "        polarity_optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=eps\n",
    "    )\n",
    "    epochs = num_train_epochs\n",
    "    max_grad_norm = 1.0\n",
    "    total_steps = epochs * len(polarity_train_dataloader)\n",
    "\n",
    "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
    "        polarity_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    epoch_step = 0\n",
    "\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        entity_property_model.train()\n",
    "        epoch_step += 1\n",
    "\n",
    "        # entity_property train\n",
    "        entity_property_total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(entity_property_train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            entity_property_model.zero_grad()\n",
    "\n",
    "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            entity_property_total_loss += loss.item()\n",
    "            # print('batch_loss: ', loss.item())\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
    "            entity_property_optimizer.step()\n",
    "            entity_property_scheduler.step()\n",
    "\n",
    "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
    "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        model_saved_path = category_extraction_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
    "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
    "\n",
    "        if do_eval:\n",
    "            entity_property_model.eval()\n",
    "\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "\n",
    "            for batch in entity_property_dev_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                pred_list.extend(predictions)\n",
    "                label_list.extend(b_labels)\n",
    "\n",
    "            evaluation(label_list, pred_list, len(tf_id_to_name))\n",
    "\n",
    "\n",
    "        # polarity train\n",
    "        polarity_total_loss = 0\n",
    "        polarity_model.train()\n",
    "\n",
    "        for step, batch in enumerate(polarity_train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            polarity_model.zero_grad()\n",
    "\n",
    "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            polarity_total_loss += loss.item()\n",
    "            # print('batch_loss: ', loss.item())\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
    "            polarity_optimizer.step()\n",
    "            polarity_scheduler.step()\n",
    "\n",
    "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
    "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        model_saved_path = polarity_classification_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n",
    "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
    "\n",
    "        if do_eval:\n",
    "            polarity_model.eval()\n",
    "\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "\n",
    "            for batch in polarity_dev_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                pred_list.extend(predictions)\n",
    "                label_list.extend(b_labels)\n",
    "\n",
    "            evaluation(label_list, pred_list, len(polarity_id_to_name))\n",
    "\n",
    "    print(\"training is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K7iPMzFjdWO"
   },
   "outputs": [],
   "source": [
    "train_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTLJM3T2jEse"
   },
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YqSiRaPjF-z"
   },
   "source": [
    "학습된 모델을 바탕으로 국어원 데이터 형태를 만드는 방법 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1662696783851,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "HDbTFCdrQbyn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_from_korean_form(tokenizer, ce_model, pc_model, data):\n",
    "\n",
    "    ce_model.to(device)\n",
    "    ce_model.eval()\n",
    "    for sentence in data:\n",
    "        form = sentence['sentence_form']\n",
    "        sentence['annotation'] = []\n",
    "        if type(form) != str:\n",
    "            print(\"form type is arong: \", form)\n",
    "            continue\n",
    "        for pair in entity_property_pair:\n",
    "            \n",
    "\n",
    "            tokenized_data = tokenizer(form, pair, padding='max_length', max_length=256, truncation=True)\n",
    "\n",
    "            input_ids = torch.tensor([tokenized_data['input_ids']]).to(device)\n",
    "            attention_mask = torch.tensor([tokenized_data['attention_mask']]).to(device)\n",
    "            with torch.no_grad():\n",
    "                _, ce_logits = ce_model(input_ids, attention_mask)\n",
    "\n",
    "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
    "\n",
    "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
    "\n",
    "            if ce_result == 'True':\n",
    "                with torch.no_grad():\n",
    "                    _, pc_logits = pc_model(input_ids, attention_mask)\n",
    "\n",
    "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
    "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
    "\n",
    "                sentence['annotation'].append([pair, pc_result])\n",
    "\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo8V29G0jONu"
   },
   "source": [
    "F1 score 계산 - 추출 성능 및 전체 성능에 대한 F1 score 따로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1662696754477,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "JrZfT2gDQb2o"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluation_f1(true_data, pred_data):\n",
    "\n",
    "    true_data_list = true_data\n",
    "    pred_data_list = pred_data\n",
    "\n",
    "    ce_eval = {\n",
    "        'TP': 0,\n",
    "        'FP': 0,\n",
    "        'FN': 0,\n",
    "        'TN': 0\n",
    "    }\n",
    "\n",
    "    pipeline_eval = {\n",
    "        'TP': 0,\n",
    "        'FP': 0,\n",
    "        'FN': 0,\n",
    "        'TN': 0\n",
    "    }\n",
    "\n",
    "    for i in range(len(true_data_list)):\n",
    "\n",
    "        # TP, FN checking\n",
    "        is_ce_found = False\n",
    "        is_pipeline_found = False\n",
    "        for y_ano  in true_data_list[i]['annotation']:\n",
    "            y_category = y_ano[0]\n",
    "            y_polarity = y_ano[2]\n",
    "\n",
    "            for p_ano in pred_data_list[i]['annotation']:\n",
    "                p_category = p_ano[0]\n",
    "                p_polarity = p_ano[1]\n",
    "\n",
    "                if y_category == p_category:\n",
    "                    is_ce_found = True\n",
    "                    if y_polarity == p_polarity:\n",
    "                        is_pipeline_found = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            if is_ce_found is True:\n",
    "                ce_eval['TP'] += 1\n",
    "            else:\n",
    "                ce_eval['FN'] += 1\n",
    "\n",
    "            if is_pipeline_found is True:\n",
    "                pipeline_eval['TP'] += 1\n",
    "            else:\n",
    "                pipeline_eval['FN'] += 1\n",
    "\n",
    "            is_ce_found = False\n",
    "            is_pipeline_found = False\n",
    "\n",
    "        # FP checking\n",
    "        for p_ano in pred_data_list[i]['annotation']:\n",
    "            p_category = p_ano[0]\n",
    "            p_polarity = p_ano[1]\n",
    "\n",
    "            for y_ano  in true_data_list[i]['annotation']:\n",
    "                y_category = y_ano[0]\n",
    "                y_polarity = y_ano[2]\n",
    "\n",
    "                if y_category == p_category:\n",
    "                    is_ce_found = True\n",
    "                    if y_polarity == p_polarity:\n",
    "                        is_pipeline_found = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            if is_ce_found is False:\n",
    "                ce_eval['FP'] += 1\n",
    "\n",
    "            if is_pipeline_found is False:\n",
    "                pipeline_eval['FP'] += 1\n",
    "            is_ce_found = False\n",
    "            is_pipeline_found = False\n",
    "\n",
    "    ce_precision = ce_eval['TP']/(ce_eval['TP']+ce_eval['FP'])\n",
    "    ce_recall = ce_eval['TP']/(ce_eval['TP']+ce_eval['FN'])\n",
    "\n",
    "    ce_result = {\n",
    "        'Precision': ce_precision,\n",
    "        'Recall': ce_recall,\n",
    "        'F1': 2*ce_recall*ce_precision/(ce_recall+ce_precision)\n",
    "    }\n",
    "\n",
    "    pipeline_precision = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FP'])\n",
    "    pipeline_recall = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FN'])\n",
    "\n",
    "    pipeline_result = {\n",
    "        'Precision': pipeline_precision,\n",
    "        'Recall': pipeline_recall,\n",
    "        'F1': 2*pipeline_recall*pipeline_precision/(pipeline_recall+pipeline_precision)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'category extraction result': ce_result,\n",
    "        'entire pipeline result': pipeline_result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpQwoVahjZoV"
   },
   "source": [
    "테스트 데이터에 대한 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1662698134351,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "Npy40TZoQlmS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_sentiment_analysis():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    test_data = jsonlload(test_data_path)\n",
    "\n",
    "    entity_property_test_data, polarity_test_data = get_dataset(test_data, tokenizer, max_len)\n",
    "\n",
    "    entity_property_test_dataloader = DataLoader(entity_property_test_data, shuffle=True,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    polarity_test_dataloader = DataLoader(polarity_test_data, shuffle=True,\n",
    "                                                  batch_size=batch_size)\n",
    "    \n",
    "    model = RoBertaBaseClassifier(len(tf_id_to_name), len(tokenizer))\n",
    "    model.load_state_dict(torch.load(test_category_extraction_model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "            \n",
    "    polarity_model = RoBertaBaseClassifier(len(polarity_id_to_name), len(tokenizer))\n",
    "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
    "    polarity_model.to(device)\n",
    "    polarity_model.eval()\n",
    "\n",
    "    pred_data = predict_from_korean_form(tokenizer, model, polarity_model, copy.deepcopy(test_data))\n",
    "\n",
    "    # jsondump(pred_data, './pred_data.json')\n",
    "    # pred_data = jsonload('./pred_data.json')\n",
    "\n",
    "    print('F1 result: ', evaluation_f1(test_data, pred_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 367446,
     "status": "ok",
     "timestamp": 1662698510136,
     "user": {
      "displayName": "정용빈",
      "userId": "11947955993359770436"
     },
     "user_tz": -540
    },
    "id": "K1QaMuWCc5jV",
    "outputId": "5b6fbfb4-121e-4102-9c64-276fc516b8c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['본품#품질', ['기어', 16, 18], 'negative']\n",
      "['본품#품질', ['기어 텐션', 67, 72], 'negative']\n",
      "['제품 전체#일반', [None, 0, 0], 'positive']\n",
      "['제품 전체#일반', ['샥이 없는 모델', 0, 8], 'neutral']\n",
      "['본품#일반', ['안장', 0, 2], 'negative']\n",
      "['제품 전체#일반', ['자전거', 23, 26], 'positive']\n",
      "['본품#품질', ['내장 기어 3단', 0, 8], 'positive']\n",
      "['본품#품질', ['UD20', 14, 18], 'negative']\n",
      "['본품#품질', ['자막 검색 후 등록 기능', 11, 24], 'negative']\n",
      "['본품#품질', ['등록]키', 3, 7], 'negative']\n",
      "['본품#품질', ['부가 기능', 3, 8], 'positive']\n",
      "['제품 전체#일반', [None, 0, 0], 'negative']\n",
      "['제품 전체#일반', [None, 0, 0], 'negative']\n",
      "['제품 전체#일반', [None, 0, 0], 'negative']\n",
      "['제품 전체#일반', [None, 0, 0], 'negative']\n",
      "F1 result:  {'category extraction result': {'Precision': 1.0, 'Recall': 0.6666666666666666, 'F1': 0.8}, 'entire pipeline result': {'Precision': 1.0, 'Recall': 0.6666666666666666, 'F1': 0.8}}\n"
     ]
    }
   ],
   "source": [
    "test_sentiment_analysis()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJjry/2Jf/MZE7Ws3nDkld",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
